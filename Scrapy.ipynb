{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Scrapy Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Workshop on Web Scraping and Text Processing with Python**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**by Radhika Saksena, Princeton University, saksena@princeton.edu, radhika.saksena@gmail.com**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Disclaimer: The code examples presented in this workshop are for educational purposes only. Please seek advice from a legal expert about the legal implications of using this code for web scraping.*"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "1. Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scrapy is a Python framework for web scraping and crawling. In just a few lines of Python code, you can crawl a website in a fairly sophisticated manner \u2013 setting up filters for pages to download, links to follow, etc. In this post, we\u2019ll see how to use Scrapy to crawl the Japanese PM\u2019s website and gather speeches from 2014."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "2. Installing Scrapy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The first step is to download Scrapy from its website and install it with ``pip``. Try running the following either with or without ``sudo``."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pip install Scrapy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "or if you want to do a system-wide install, install with ``sudo``"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sudo pip install Scrapy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3. Running a Scrapy Project"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Navigate to any directory on your computer where you\u2019d like to set up the Scrapy project. Run the ``scrapy startproject`` command as shown below to generate the KanteiPress project directory."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scrapy startproject KanteiPress"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The newly-created ``KanteiPress`` project directory will be populated with a sub-directory which is also called ``KanteiPress``  (so, you\u2019ll have directory structure: ``KanteiPress/KanteiPress``) and a ``scrapy.cfg`` file."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Now download the KanteiSpider.py file from github at https://github.com/scalezen/Montreal-Python-Web/blob/master/day1/KanteiSpider.py in to the directory KanteiPress/KanteiPress/spiders and rename it to KanteiSpider.py"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The KanteiSpider class is defined in KanteiSpider.py. The class definition includes a property called ``name`` which is set to ``Kantei``. This will be used by Scrapy to start the crawl as we shall see in a few steps."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "class KanteiSpider(CrawlSpider):\n",
      "    name = \"Kantei\"\n",
      "    allowed_domains = [\"kantei.go.jp\"]\n",
      "    start_urls = [\n",
      "    \"http://www.kantei.go.jp/foreign/96_abe/statement/index_e.html\"\n",
      "    ]\n",
      "    ..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The ``rules`` attribute in KanteiSpider.py is important as this is where we\u2019ll specify actions that need to be performed when we encounter links during the crawl that specific patterns. The rules are processed sequentially and the first rule that matches is the one that is obeyed."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "rules = (\n",
      "    Rule(SgmlLinkExtractor(allow=(re.compile(\"2013.*\\/index.*\\.html\"))),callback=\u2019parse_item\u2019,follow=True),\n",
      "    Rule(SgmlLinkExtractor(allow=(re.compile(\"2013.*\\/.*\\.html\"))),callback=\u2019press_item\u2019,follow=False),\n",
      "    Rule(SgmlLinkExtractor(allow=(re.compile(\"2013\"))),callback=\u2019parse_item\u2019,follow=True),)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* In the code snippet (from KanteiSpider.py) shown above, the first rule specifies that, during the crawl, if we encounter a link that contains the pattern ``2013`` followed by some text and then ``/index.html``, then call the method ``parse_item()`` and follow that link as the crawl continues."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The ``parse_item()`` callback method does nothing except it prints the URL of the link."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The second rule specifies that if we encounter a link that contains the pattern ``2013`` followed by some text and then ``/`` followed by a filename that ends with .html, then call the method ``press_item()`` and do not follow that link. We choose not to follow such a link as it is an html file and most likely it is one of those press items/speeches that we are after \u2013 the crawl could safely be stopped for this branch of the depth-first/bread-first search. Alternatively, in this case, one could also set the ``follow`` attribute to true in this rule and that wouldn\u2019t affect the outcome too much."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The ``press_item()`` callback method prints the link. It then extracts the name of the linked file from the URL. The linked file is then downloaded and saved in a directory called ``archives``."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The third rule specifies that if the link contains the pattern ``2013`` then the ``parse_item()`` function is called and the link should be followed as the crawl continues."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Once, we have set-up the Scrapy project including the code for the crawl spider (KanteiSpider.py), we can run the Kantei crawl by executing the following command in the KanteiPress project directory."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scrapy crawl Kantei"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Sample output from the crawl is shown below. A directory called \u201carchives\u201d would have been generated in the location from where the crawl was executed. All of the 2014 speeches will be available in \u201carchives\u201d at the end of the crawl."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "parse_item: Next link is http://www.kantei.go.jp/foreign/96_abe/statement/201311/index.html\n",
      "parse_item: Next link is http://www.kantei.go.jp/foreign/96_abe/statement/201307/index.html\n",
      "2014-01-13 22:30:01-0500 [Kantei] DEBUG: Crawled (200) <GET http://www.kantei.go.jp/foreign/96_abe/statement/201303/index.html> (referer: http://www.kantei.go.jp/foreign/96_abe/statement/index_e.html)\n",
      "2014-01-13 22:30:01-0500 [Kantei] DEBUG: Crawled (200) <GET http://www.kantei.go.jp/foreign/96_abe/statement/201312/06danwa_e.html> (referer: http://www.kantei.go.jp/foreign/96_abe/statement/index_e.html)\n",
      "....\n",
      "parse_item: Next link is http://www.kantei.go.jp/foreign/96_abe/statement/201303/index.html\n",
      "press_item: Saved link is http://www.kantei.go.jp/foreign/96_abe/statement/201312/06danwa_e.html\n",
      "Saving file archives/www.kantei.go.jp_foreign_96_abe_statement_201312_06danwa_e.html."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}