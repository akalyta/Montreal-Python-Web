{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Handling Multiple Languages with Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**by Radhika Saksena <br/>\n",
      "Princeton University <br/>\n",
      "saksena@princeton.edu, radhika.saksena@gmail.com**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Disclaimer: The code examples presented in this and subsequent handouts are for educational purposes only. Please seek advice from a legal expert about the legal implications of using this code for web scraping.\n",
      "**"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "1. Regular Expressions and Web Scraping in multiple languages"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this section, we will extend the use of web scraping tools to non-English language content. We will learn about:\n",
      "\n",
      "* Using Python's regular expressions module ``re`` and BeautifulSoup wwith non-English characters.\n",
      "* Using BeautifulSoup to scrape non-English websites.\n",
      "* Examples include French and Japanese content - extensible to any language."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1.1. Some Background"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A character encoding scheme is a map between a character set and \n",
      "the underlying machine representation of its characters. One of the\n",
      "most intuitive encoding scheme to consider for English content is \n",
      "American Standard Code for Information Interchange (ASCII) encoding\n",
      "which maps 128 specified characters (which include the alphabet, \n",
      "numbers, punctuations, etc.) into 7-bit patterns. Without any \n",
      "user-customization, Python's default encoding scheme is ASCII.\n",
      "\n",
      "When considering di\ufb00erent languages, the number of characters that \n",
      "one needs to deal with exceeeds those within the scope of ASCII \n",
      "encoding and in these situations encodings other than ASCII are used. \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Ex: Japanese Mission Statements"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For example, the list of statements published by the Japanese mission\n",
      "to the U.N. are available at http://www.un.emb-japan.go.jp/jp/statements/2013.html.\n",
      "A snippet of the HTML source is shown below. In the HTML source we can\n",
      "see that the <meta> tag has an attribute ``charset`` that speci\ufb01es \n",
      "the encoding, ``utf-8``. This encoding is used in the rest of the HTML document. \n",
      "\n",
      "Python\u2019s support for di\ufb00erent encodings must be leveraged to handle\n",
      "non-ASCII characters.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"ja-JP\" lang=\"ja-JP\"><!-- InstanceBegin template=\"/Templates/main.dwt\" codeOutsideHTMLIsLocked=\"false\" -->\n",
      "\n",
      "<head>\n",
      "<!-- InstanceBeginEditable name=\"doctitle\" -->\n",
      "<title>\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8 |  \u56fd\u969b\u9023\u5408\uff08\u56fd\u9023\uff09 \u65e5\u672c\u653f\u5e9c\u4ee3\u8868\u90e8</title>\n",
      "<meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" />\n",
      "<meta name=\"description\" content=\"\u5b89\u5168\u4fdd\u969c\u7406\u4e8b\u4f1a\u3092\u306f\u3058\u3081\u3068\u3057\u305f\u56fd\u9023\u306e\u53d6\u308a\u7d44\u307f\u306b\u304a\u3051\u308b\u5927\u4f7f\u306e\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u3092\u8a9e\u3089\u306b\u306a\u308c\u307e\u3059\u3002\" />\n",
      "<meta name=\"keywords\" content=\"\u56fd\u9023, \u56fd\u969b\u9023\u5408, \u65e5\u672c, \u5916\u4ea4, \u5b89\u4fdd\u7406, \u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\">\n",
      "<link rel=\"stylesheet\" href=\"../../css/accordionmenu.css\" type=\"text/css\" media=\"screen\" />\n",
      "\n",
      "...\n",
      "\n",
      "<table width=\"97%\"  cellpadding=\"8px\" cellspacing=\"0px\" class=\"list_borderbtm\">  \n",
      "\n",
      "\t<tr>\n",
      "\t  <th width=\"38%\" bgcolor=\"#eeeeee\" >\u30bf\u30a4\u30c8\u30eb</th>\n",
      "\t  <th width=\"13%\" bgcolor=\"#eeeeee\">\u30b9\u30d4\u30fc\u30ab\u30fc</th>\n",
      "\t  <th width=\"20%\" bgcolor=\"#eeeeee\">\u4f1a\u8b70</th>\n",
      "\t  <th width=\"17%\" bgcolor=\"#eeeeee\">\u9805\u76ee</th>\n",
      "\t  <th width=\"12%\" bgcolor=\"#eeeeee\">\u65e5\u4ed8</th>\n",
      "\t  </tr>\n",
      "\t<tr>\n",
      "\t  <td><p><a href=\"yamazaki122713.html\">\u7b2c\uff16\uff18\u56de\u56fd\u9023\u7dcf\u4f1a\u7b2c\uff15\u59d4\u54e1\u4f1a\u4e3b\u8981\u4f1a\u671f\u9589\u4f1a\u306b\u3042\u305f\u3063\u3066\u306e\u5c71\u5d0e\u5927\u4f7f\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8</a></p></td>\n",
      "\t  <td>\u5c71\u5d0e\u7d14\u5927\u4f7f</td>\n",
      "\t  <td>\u7b2c5\u59d4\u54e1\u4f1a</td>\n",
      "\t  <td>\u56fd\u9023\u8ca1\u653f\u554f\u984c</td>\n",
      "\t  <td>2013\u5e7412\u670827\u65e5</td>\n",
      "\t  </tr>\n",
      "\t<tr>\n",
      "\t  <td><a href=\"umemoto121913.html\">\u30de\u30f3\u30c7\u30e9\u5143\u5357\u30a2\u30d5\u30ea\u30ab\u5927\u7d71\u9818\u306e\u305f\u3081\u306e\u56fd\u9023\u7dcf\u4f1a\u7279\u5225\u4f1a\u5408\u306b\u304a\u3051\u308b\u6885\u672c\u5927\u4f7f\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\uff08\u82f1\u6587\uff09</a></td>\n",
      "\t  <td>\u6885\u672c\u548c\u7fa9\u5927\u4f7f</td>\n",
      "\t  <td>\u30de\u30f3\u30c7\u30e9\u5143\u5357\u30a2\u30d5\u30ea\u30ab\u5927\u7d71\u9818\u306e\u305f\u3081\u306e\u56fd\u9023\u7dcf\u4f1a\u7279\u5225\u4f1a\u5408</td>\n",
      "\t  <td>\u305d\u306e\u4ed6</td>\n",
      "\t  <td>2013\u5e7412\u670819\u65e5</td>\n",
      "\t  </tr>\n",
      "\t<tr>\n",
      "...\n",
      "\n",
      "</table>"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1.2. Using Unicode strings in Python code"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We will focus on Unicode strings (http://www.unicode.org/versions/Unicode6.3.0/) and UTF-8 encoding\n",
      "in particular as this is increasingly becoming common for online content. In addition to UTF-8, the ISO-\n",
      "8859-1/Latin-1 encoding is also very common for Western European language content. The fundamentals of\n",
      "working with encodings don\u2019t change and having a good handle on how to work with one encoding scheme,\n",
      "e.g., UTF-8, will be useful for text processing tasks in other character encodings.\n",
      "\n",
      "\n",
      "\n",
      "* Since there is built-in support for Unicode in Python, the interface to handling Unicode strings is very similar\n",
      "to that of ASCII strings. Syntax for assignment (``=`` operator), ``join()``, ``replace()``, \n",
      "string comparison, etc. remains the same. One key di\ufb00erence is that Unicode literals are speci\ufb01ed with \n",
      "a ``u`` pre\ufb01x.\n",
      "\n",
      "\n",
      "* Unicode literals are supported in Python source code. However, in order to specify Unicode literals, a\n",
      "declaration ``#coding: utf-8`` must be speci\ufb01ed at the beginning of the Python script. The name of the\n",
      "encoding can be whatever is in use in the Python code, i.e., one could use #coding: latin-1, etc.\n",
      "BeautifulSoup handles UTF-8 encoding transparently.\n",
      "\n",
      "\n",
      "* As shown in example code below, we can perform regular expression searches with BeautifulSoup for a Unicode string literal just as we did with ASCII strings. The example searches for specific strings on the Japanese Mission's web-page ((http://www.un.emb-japan.go.jp/jp/statements/2013.html)) that lists all the statements made in 2013."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#coding: utf-8\n",
      "import re\n",
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "searchStrAfg = u\"\u30a2\u30d5\u30ac\u30cb\u30b9\u30bf\u30f3\"\n",
      "searchStrNK = u\"\u5317\u671d\u9bae\"\n",
      "\n",
      "patternAfg = re.compile(searchStrAfg)\n",
      "patternNK = re.compile(searchStrNK)\n",
      "\n",
      "soup = BeautifulSoup(urllib2.urlopen(\"http://www.un.emb-japan.go.jp/jp/statements/2013.html\").read())\n",
      "\n",
      "afgTags = soup.find_all(text=patternAfg)\n",
      "NKTags = soup.find_all(text=patternNK)\n",
      "\n",
      "print \"Total number of occurrences of Afghanistan on the web page is {0:d}\".format(len(afgTags))\n",
      "print \"Total number of occurrences of North Korea on the web page is is {0:d}\".format(len(NKTags))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total number of occurrences of Afghanistan on the web page is 5\n",
        "Total number of occurrences of North Korea on the web page is is 1\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A more principled approach to searching for these strings in the **titles** of **all** the statements made by the Japanese Mission from 2002 to 2014 is demonstrated in the Python script ``searchJapaneseText.py``. Load and run this script to see how it works."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load searchJapaneseText.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Copyright 2014, Radhika S. Saksena (radhika dot saksena at gmail dot com)\n",
      "# Licensed under the Apache License, Version 2.0\n",
      "# http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "# Web scraping and text processing with Python workshop\n",
      "\n",
      "import re\n",
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "\"\"\"Count the frequency of specific unicode patterns in all the statements\n",
      "    made by the Permanent Japanese Mission to the U.N. in the period 2002-2014.\"\"\"\n",
      "\n",
      "if __name__==\"__main__\":\n",
      "\n",
      "    searchStrAfg = u\"\u30a2\u30d5\u30ac\u30cb\u30b9\u30bf\u30f3\"\n",
      "    searchStrNK = u\"\u5317\u671d\u9bae\"\n",
      "\n",
      "    patternAfg = re.compile(searchStrAfg)\n",
      "    patternNK = re.compile(searchStrNK)\n",
      "\n",
      "    countAfg = 0\n",
      "    countNK = 0\n",
      "\n",
      "    for i in range(2002,2014):\n",
      "        url = \"http://www.un.emb-japan.go.jp/jp/statements/%s.html\" % i\n",
      "        print \"Fetching url %s.....\" % url\n",
      "        html = urllib2.urlopen(url).read()\n",
      "\n",
      "        soup = BeautifulSoup(html)\n",
      "\n",
      "        \"\"\"Count the number of references to Afghanistan and North Korea in the document.\"\"\"\n",
      "        rowTags = soup.find_all(\"tr\")\n",
      "        for rowTag in rowTags:\n",
      "            colTags = rowTag.find_all(\"td\")\n",
      "            if(colTags):\n",
      "                colTag = colTags[0]\n",
      "                afgTag = colTag.find(text=patternAfg)\n",
      "                if(afgTag):\n",
      "                    countAfg += 1\n",
      "                NKTag = colTag.find(text=patternNK)\n",
      "                if(NKTag):\n",
      "                    countNK += 1\n",
      "\n",
      "    print \"Total number of Statements with Afghanistan in the title is %d\" % countAfg\n",
      "    print \"Total number of Statments with North Korea in the title is %d\" % countNK\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fetching url http://www.un.emb-japan.go.jp/jp/statements/2002.html.....\n",
        "Fetching url http://www.un.emb-japan.go.jp/jp/statements/2003.html....."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Fetching url http://www.un.emb-japan.go.jp/jp/statements/2004.html....."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Fetching url http://www.un.emb-japan.go.jp/jp/statements/2005.html....."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Fetching url http://www.un.emb-japan.go.jp/jp/statements/2006.html....."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Fetching url http://www.un.emb-japan.go.jp/jp/statements/2007.html....."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Fetching url http://www.un.emb-japan.go.jp/jp/statements/2008.html....."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Fetching url http://www.un.emb-japan.go.jp/jp/statements/2009.html....."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Fetching url http://www.un.emb-japan.go.jp/jp/statements/2010.html....."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Fetching url http://www.un.emb-japan.go.jp/jp/statements/2011.html....."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Fetching url http://www.un.emb-japan.go.jp/jp/statements/2012.html....."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Fetching url http://www.un.emb-japan.go.jp/jp/statements/2013.html....."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Total number of Statements with Afghanistan in the title is 19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Total number of Statments with North Korea in the title is 4\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1.3. Encoding and Decoding Unicode Strings"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Although you might not need to do this too often, it is useful to know that one can also encode and decode\n",
      "strings in the various encodings as shown in the code below. The encode method returns a machine representation\n",
      "- byte string version of the Unicode string in the requested encoding. Let\u2019s try encoding the Unicode string,\n",
      "``\u30a2\u30d5\u30ac\u30cb\u30b9\u30bf\u30f3``, in UTF-8, ASCII and Latin-1 encoding. Also, try encoding ``cote d\u2019ivoire`` in Latin-1.\n",
      "The decode method converts each byte in the byte string to the appropriate character using the character (UTF-8, latin-1, etc.) encoding scheme speci\ufb01ed as its argument."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load testEncodeDecode.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Copyright 2014, Radhika S. Saksena (radhika dot saksena at gmail dot com)\n",
      "# Licensed under the Apache License, Version 2.0\n",
      "# http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "# Web scraping and text processing with Python workshop\n",
      "\n",
      "import sys\n",
      "\n",
      "\"\"\"Test exercises to encode and decode Unicode strings.\"\"\"\n",
      "\n",
      "if __name__==\"__main__\": \n",
      "    strAfg = u\"\u30a2\u30d5\u30ab\u3099\u30cb\u30b9\u30bf\u30f3\" \n",
      "    strCDI = u\"cote d\u2019ivoire\"\n",
      "\n",
      "    print(strAfg.encode(\"utf-8\"))\n",
      "\n",
      "    \"\"\"Does this work?\"\"\"\n",
      "    utf8Enc = strAfg.encode(\"utf-8\",errors=\"strict\")\n",
      "\n",
      "    \"\"\"Does this work? Uncomment and try it.\"\"\"\n",
      "    #strAfg.encode(\"ascii\",errors=\"strict\")\n",
      "    #strAfg.encode(\"ascii\",errors=\"ignore\")\n",
      "    #strAfg.encode(\"ascii\",errors=\"replace\")\n",
      "\n",
      "    \"\"\"Does this work? Uncomment and try it.\"\"\"\n",
      "    #strAfg.encode(\"latin-1\",errors=\"strict\")\n",
      "    #strCDI.encode(\"latin-1\",errors=\"strict\")\n",
      "\n",
      "    \"\"\"Try decoding\"\"\"\n",
      "    #utf8Enc.decode(\"utf-8\")\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u30a2\u30d5\u30ab\u3099\u30cb\u30b9\u30bf\u30f3\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Writing and Reading files in Multiple Languages"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to read and write non-ASCII content to \ufb01les, one possible way is to use the ``codecs`` module\n",
      "functionality. \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Ex: Japanese Mission Statements (cont.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The example below demonstrates the use of the codecs module. In this example:\n",
      "\n",
      "\n",
      "* First, we scrape the links to the 2013 statements of the Japanese Mission from the list available at http://www.un.emb-japan.go.jp/jp/statements/2013.html . \n",
      "\n",
      "\n",
      "* Then we follow the links to download the full text of each of the statements. One of the speech texts is available as PDF documents - in this case, we don't have to worry about explicitly writing the HTTP response to local disk and just use the ``urllib.urlretrieve()`` method to download the file. The rest of the full speech texts are available in HTML format in either English or Japanese and the code to write the downloaded content to local file works seamlessly for both languages using the codecs module. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load getJapanStatements.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Copyright 2014, Radhika S. Saksena (radhika dot saksena at gmail dot com)\n",
      "# Licensed under the Apache License, Version 2.0\n",
      "# http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "# Web scraping and text processing with Python workshop\n",
      "\n",
      "\n",
      "import re\n",
      "import codecs\n",
      "import urllib2\n",
      "import urllib\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "\"\"\"Retrieve all the 2013 statements made by the Permanent Japanese Mission to the U.N.\"\"\"\n",
      "\n",
      "if __name__==\"__main__\":\n",
      "\n",
      "            prefix = \"http://www.un.emb-japan.go.jp/jp/statements\"\n",
      "            url = \"http://www.un.emb-japan.go.jp/jp/statements/2013.html\"\n",
      "            html = urllib2.urlopen(url).read()\n",
      "\n",
      "            soup = BeautifulSoup(html)\n",
      "            \"\"\"Find the tag object corresponding to the table element\"\"\"\n",
      "            #table = soup.find(text=re.compile(u\"\u30bf\u30a4\u30c8\u30eb\")).parent.parent.parent\n",
      "            table = soup.find(\"table\",width=\"97%\")\n",
      "\n",
      "            \"\"\"Go through each row and fetch the link in the first column.\"\"\"\n",
      "            rowTags = table.find_all(\"tr\")\n",
      "            for rowTag in rowTags:\n",
      "                colTags = rowTag.find_all(\"td\")\n",
      "                if(colTags):\n",
      "                    linkURLs = colTags[0].find_all(\"a\")\n",
      "\n",
      "                    # get statement in second language (non-English) if available\n",
      "                    if(len(linkURLs) > 1):\n",
      "                        linkURL = linkURLs[1].get(\"href\")\n",
      "                    else:\n",
      "                        linkURL = linkURLs[0].get(\"href\")\n",
      "\n",
      "                    # check if it is a PDF file or HTML file\n",
      "                    if(re.search(\"\\.pdf\",linkURL)):\n",
      "                        linkURL = re.sub(\".*\\/statements\\/([A-Za-z].*\\.pdf)\",\"\\g<1>\",linkURL)\n",
      "                        PDFName = linkURL\n",
      "                        print(\"Downloading file: {0}/{1}.\".format(prefix,linkURL))\n",
      "                        urllib.urlretrieve(\"{0}/{1}\".format(prefix,linkURL),\n",
      "                                                PDFName)\n",
      "                    else:\n",
      "                        linkURL = re.sub(\".*\\/statements\\/([A-Za-z].*\\.(html))\",\"\\g<1>\",linkURL)\n",
      "                        statement = urllib2.urlopen(\"%s/%s\" % (prefix,linkURL)).read()\n",
      "                        localFile = linkURL\n",
      "                        print(\"Downloading file: {0}/{1}.\".format(prefix,linkURL))\n",
      "                        fout = codecs.open(localFile,\"w\",\"utf-8\")\n",
      "                        fout.write(statement.decode(\"utf-8\"))\n",
      "                        fout.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3. Handling ``UnicodeEncodeError/UnicodeDecodeError``"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, there is always a possibility that some of the scraped online content may be malformed. For example, there could be errors in the web page which would lead Python encoding/decoding operations to give errors when an invalid byte sequence is encountered. So, it is always a good idea to enclose Unicode-related code in ``try: except: `` blocks and gracefully catch exceptions of type ``UnicodeEncodeError`` or ``UnicodeDecodeError``."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
